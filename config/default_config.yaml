# 联邦学习Token选择器水印系统配置文件

# 顶层数据路径，用于联邦学习客户端
data_paths:
  - "/home/qty/code/federated_token_selector/data/c4_sample.jsonl"

# 实验和环境配置
experiment:
  name: "federated_watermarking_llama3.2_1B"
  description: "使用Llama-3.2-1B进行联邦水印学习的实验"
  device: "cuda:0"  # <--- 在这里设置全局训练设备，例如 'cuda:0', 'cuda:1'
  seed: 42
  output_dir: "./outputs"
  save_frequency: 20
  log_level: "INFO"
  mixed_precision: false

# 模型配置
model:
  vocab_size: 128256  # <--- 修改：与Llama-3模型的词汇表大小匹配
  hidden_dim: 256
  top_p: 0.92
  max_candidate_tokens: 50
  num_attention_heads: 8
  num_transformer_layers: 2
  dropout: 0.1

# 水印配置
watermark:
  gamma: 0.25
  hash_key: 15485863
  z_threshold: 4.0
  bit_length: 32

# 联邦学习配置
federated:
  num_clients: 2
  num_rounds: 5000
  client_fraction: 1.0
  aggregation_method: "fedavg"
  clients:
    - client_id: 0
      llm_config:
        enabled: true
        type: "huggingface"
        huggingface:
          model_name: "meta-llama/Llama-3.2-1B"  # <-- 保持模型在Hugging Face上的名称
          cache_dir: "/home/qty/code/llama"  # <-- 指定包含此模型的缓存目录
          device: "cuda:0"
    - client_id: 1
      llm_config:
        enabled: true
        type: "huggingface"
        huggingface:
          model_name: "meta-llama/Llama-3.2-1B"  # <-- 保持模型在Hugging Face上的名称
          cache_dir: "/home/qty/code/llama"  # <-- 指定包含此模型的缓存目录
          device: "cuda:0"
  fedprox:
    mu: 0.01

# 训练配置
training:
  learning_rate: 1.0e-4
  batch_size: 32
  local_epochs: 100
  samples_per_client: 1000 # 确保这个值是1000
  loss_weights:
    watermark: 1.0
    semantic: 0.5
    fluency: 0.3
  optimizer:
    type: "adam"
    weight_decay: 1e-5
    betas: [0.9, 0.999]
  scheduler:
    type: "cosine"
    warmup_steps: 100
    min_lr: 1e-6

# 数据配置
data:
  max_seq_len: 128
  min_seq_len: 50
  synthetic_data: false
  real_data:
    dataset_name: "c4"
    data_path: "/home/qty/code/federated_token_selector/data/c4_sample.jsonl"
    tokenizer_name: "meta-llama/Llama-3.2-1B"

# 评估配置
evaluation:
  eval_frequency: 10
  test_samples: 500
  metrics:
    - "watermark_accuracy"
    - "semantic_similarity"
    - "fluency_score"
    - "detection_accuracy"
    - "false_positive_rate"

# 可视化配置
visualization:
  plot_training_curves: true
  plot_detection_stats: true
  plot_z_score_distribution: true
  save_plots: true
  plot_format: "png"

# 注意：LLM配置现在在每个客户端中单独配置
# 请参考 federated.clients 部分的 llm_config 配置

# 安全配置
security:
  differential_privacy: false
  dp_noise_multiplier: 1.0
  dp_max_grad_norm: 1.0
  
  # 模型压缩（减少通信开销）
  model_compression:
    enabled: false
    method: "quantization"  # quantization, pruning
    compression_ratio: 0.5

# 高级配置
advanced:
  # 客户端异构性模拟
  client_heterogeneity:
    enabled: false
    data_distribution: "iid"  # iid, non_iid
    alpha: 0.5  # Dirichlet分布参数（用于non-IID）
  
  # 系统异构性
  system_heterogeneity:
    enabled: false
    computation_variance: 0.2
    communication_variance: 0.1
  
  # 拜占庭容错
  byzantine_robustness:
    enabled: false
    byzantine_fraction: 0.1
    defense_method: "krum"  # krum, trimmed_mean, median

# 扩展实验配置
extensions:
  # 联邦蒸馏
  federated_distillation:
    enabled: false
    teacher_model_path: null
    distillation_temperature: 3.0
    distillation_weight: 0.5
  
  # 联邦对抗训练
  federated_adversarial:
    enabled: false
    adversarial_epsilon: 0.1
    adversarial_steps: 5
  
  # 多任务学习
  multi_task:
    enabled: false
    tasks:
      - "watermark_embedding"
      - "text_quality"
      - "semantic_preservation"
    task_weights: [1.0, 0.5, 0.5]