# 预处理数据联邦学习配置文件

experiment:
  name: "federated_watermarking_preprocessed"
  description: "使用预处理token数据进行联邦水印学习的实验"
  device: "cuda:0"
  seed: 42
  output_dir: "./outputs_preprocessed"

model:
  vocab_size: 128256
  hidden_dim: 1600  # 使用一个统一的维度，或者最大的维度
  top_p: 0.92
  max_candidate_tokens: 50
  num_attention_heads: 8
  num_transformer_layers: 2
  dropout: 0.1

watermark:
  gamma: 0.25
  hash_key: 15485863
  bit_length: 32

federated:
  num_clients: 3
  num_rounds: 100
  client_fraction: 1.0
  aggregation_method: "fedavg"
  clients:
    - client_id: 0
      data_path: "/home/qty/code/federated_token_selector/processed_data/output_token/c4_gpt2-xl.jsonl"
      model_name: "gpt2-xl"
      cache_dir: "/home/qty/code/gpt2-xl"
      interface_type: "huggingface"
      model_hidden_dim: 1600  # GPT2-XL的隐藏层维度
    - client_id: 1
      data_path: "/home/qty/code/federated_token_selector/processed_data/output_token/c4_llama1b.jsonl"
      model_name: "meta-llama/Llama-3.2-1B"
      cache_dir: "/home/qty/code/llama"
      interface_type: "huggingface"
      model_hidden_dim: 2048  # Llama-3.2-1B的隐藏层维度
    - client_id: 2
      data_path: "/home/qty/code/federated_token_selector/processed_data/output_token/c4_opt-1.3b.jsonl"
      model_name: "facebook/opt-1.3b"
      cache_dir: "/home/qty/code/opt-1.3b"
      interface_type: "huggingface"
      model_hidden_dim: 2048  # OPT-1.3B的隐藏层维度

training:
  learning_rate: 1.0e-4
  batch_size: 32
  local_epochs: 5
  samples_per_client: 1000
  loss_weights:
    watermark: 1.0
    semantic: 0.5
    fluency: 0.3